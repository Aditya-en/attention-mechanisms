{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e822deb-c0c4-4019-b7af-e2ab9b74219f",
   "metadata": {},
   "source": [
    "## Multi Head Attention\n",
    "The original attention mechanism as introduced in the seminal paper [Vaswani et al. (2017), \"Attention Is All You Need\"](https://arxiv.org/abs/1706.03762). It splits the latent dimension in many heads that learns unique patterns by creating its own queries, keys and values. It is used in transformer models like the Bert and GPT.\n",
    "![Multi Head Attention](./assets/mha.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bbb47dc-82ce-45a0-9746-7d3756e74405",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9159f859-a41d-41cb-a37d-b0dbd39e4945",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_dim,\n",
    "        num_heads,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "        self.query = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.keys = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.values = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.output = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, s, d = x.shape\n",
    "        query = self.query(x)\n",
    "        keys = self.keys(x)\n",
    "        values = self.values(x)\n",
    "        \n",
    "        query = query.view(b, s, self.num_heads, self.head_dim)\n",
    "        keys = keys.view(b, s, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, s, self.num_heads, self.head_dim)\n",
    "\n",
    "        query = query.transpose(1,2)\n",
    "        keys = keys.transpose(1,2)\n",
    "        values = values.transpose(1,2)\n",
    "        \n",
    "        atten_score = torch.softmax((query@keys.transpose(2,3))/(self.head_dim**0.5), dim=-1)\n",
    "        \n",
    "        out = atten_score@values\n",
    "        out = out.transpose(1,2)\n",
    "        out = out.contiguous().view(b,s,d)\n",
    "        return self.output(out)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c90dab8-52bb-4760-b5c6-afaeac155d40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 64, 256])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = MultiHeadAttention(256, 8)\n",
    "x = torch.randn((128, 64, 256))\n",
    "layer(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e39eb17-0ee1-44ef-8915-8b8b6046c36f",
   "metadata": {},
   "source": [
    "## Group Query Attention\n",
    "It is much more efficient than that of Multi Head Attention in terms of compute as in the paper [Ainslie et al. (2023), \"GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints\"](https://arxiv.org/abs/2305.13245).\n",
    "It is used in efficient large models like the llama models.\n",
    "![Group Query Attention](./assets/GQA.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95e27409-6615-4013-9a51-7aa79774f333",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9c2a0494-005b-4e84-8fc4-d98d86af1d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupQueryAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_dim,\n",
    "        query_heads,\n",
    "        kv_heads\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.head_dim = hidden_dim // query_heads\n",
    "        self.q_heads = query_heads\n",
    "        self.kv_heads = kv_heads\n",
    "        self.query = nn.Linear(hidden_dim, self.q_heads*self.head_dim)\n",
    "        self.keys = nn.Linear(hidden_dim, self.kv_heads*self.head_dim)\n",
    "        self.values = nn.Linear(hidden_dim, self.kv_heads*self.head_dim)\n",
    "        self.output = nn.Linear(self.kv_heads*self.head_dim, hidden_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b, s, d = x.shape\n",
    "        query = self.query(x)\n",
    "        keys = self.keys(x)\n",
    "        values = self.values(x)\n",
    "\n",
    "        query = query.view(b, s, self.q_heads, self.head_dim)\n",
    "        keys = keys.view(b, s, self.kv_heads, self.head_dim)\n",
    "        values = values.view(b, s, self.kv_heads, self.head_dim)\n",
    "        \n",
    "        query = query.transpose(1, 2)\n",
    "        query = query.view(b, self.q_heads//self.kv_heads, self.kv_heads, s, self.head_dim)\n",
    "        keys = keys.transpose(1, 2).unsqueeze(1)\n",
    "        values = values.transpose(1, 2)\n",
    "        \n",
    "        atten_score = torch.sum(query@keys.transpose(3,4), dim=1)\n",
    "        out = atten_score@values\n",
    "        out = out.transpose(1,2).contiguous().view(b, s, self.kv_heads*self.head_dim)\n",
    "        return self.output(out)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "17538377-f3c9-474a-b5a6-7ff9e45e1ee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 128, 256])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GroupQueryAttention(256, 8, 2)\n",
    "x = torch.randn((16, 128, 256))\n",
    "out = model(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208f3762-2e6e-4e81-b53a-255c51386564",
   "metadata": {},
   "source": [
    "## Multi Query Attention\n",
    "It is the fastest among the above attention mechanisms but comes with a trade off of quality. It was introduced in the paper [Shazeer (2019), \"Fast Transformer Decoding: One Write-Head is All You Need\"](https://arxiv.org/abs/1911.02150)\n",
    "It is a special case of Group Query Attention where the number of keys heads and value heads are equal to 1.\n",
    "![Multi Query Attention](./assets/MQA.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f17a8b7b-96ef-4428-86a9-2ea299f0fc36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 128, 256])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GroupQueryAttention(256, 8, 1)\n",
    "x = torch.randn(16, 128, 256)\n",
    "out = model(x)\n",
    "out.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
