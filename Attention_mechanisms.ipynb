{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ff41354-dfd8-4d32-a9cf-4dba500a57ab",
   "metadata": {},
   "source": [
    "# Attention Mechanisms Implemented from Scratch in PyTorch\n",
    "Minimalistic implementation of attention blocks in deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b273245c-5ce5-490f-b824-8fbe67327236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.13.1\n",
      "torch version: 2.6.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from platform import python_version\n",
    "print(\"Python version:\", python_version())\n",
    "print(\"torch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e822deb-c0c4-4019-b7af-e2ab9b74219f",
   "metadata": {},
   "source": [
    "## Multi Head Attention\n",
    "The original attention mechanism as introduced in the seminal paper [Vaswani et al. (2017), \"Attention Is All You Need\"](https://arxiv.org/abs/1706.03762). It splits the latent dimension in many heads that learns unique patterns by creating its own queries, keys and values. It is used in transformer models like the Bert and GPT.\n",
    "![Multi Head Attention](./assets/mha.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bbb47dc-82ce-45a0-9746-7d3756e74405",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9159f859-a41d-41cb-a37d-b0dbd39e4945",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_dim,\n",
    "        num_heads,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "        self.query = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.keys = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.values = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.output = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, s, d = x.shape\n",
    "        query = self.query(x)\n",
    "        keys = self.keys(x)\n",
    "        values = self.values(x)\n",
    "        \n",
    "        query = query.view(b, s, self.num_heads, self.head_dim)\n",
    "        keys = keys.view(b, s, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, s, self.num_heads, self.head_dim)\n",
    "\n",
    "        query = query.transpose(1,2)\n",
    "        keys = keys.transpose(1,2)\n",
    "        values = values.transpose(1,2)\n",
    "        \n",
    "        atten_score = torch.softmax((query@keys.transpose(2,3))/(self.head_dim**0.5), dim=-1)\n",
    "        \n",
    "        out = atten_score@values\n",
    "        out = out.transpose(1,2)\n",
    "        out = out.contiguous().view(b,s,d)\n",
    "        return self.output(out)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c90dab8-52bb-4760-b5c6-afaeac155d40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 64, 256])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = MultiHeadAttention(256, 8)\n",
    "x = torch.randn((128, 64, 256))\n",
    "layer(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e39eb17-0ee1-44ef-8915-8b8b6046c36f",
   "metadata": {},
   "source": [
    "## Group Query Attention\n",
    "It is much more efficient than that of Multi Head Attention in terms of compute as in the paper [Ainslie et al. (2023), \"GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints\"](https://arxiv.org/abs/2305.13245).\n",
    "It is used in efficient large models like the llama models.\n",
    "![Group Query Attention](./assets/GQA.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95e27409-6615-4013-9a51-7aa79774f333",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9c2a0494-005b-4e84-8fc4-d98d86af1d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupQueryAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_dim,\n",
    "        query_heads,\n",
    "        kv_heads\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.head_dim = hidden_dim // query_heads\n",
    "        self.q_heads = query_heads\n",
    "        self.kv_heads = kv_heads\n",
    "        self.query = nn.Linear(hidden_dim, self.q_heads*self.head_dim)\n",
    "        self.keys = nn.Linear(hidden_dim, self.kv_heads*self.head_dim)\n",
    "        self.values = nn.Linear(hidden_dim, self.kv_heads*self.head_dim)\n",
    "        self.output = nn.Linear(self.kv_heads*self.head_dim, hidden_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b, s, d = x.shape\n",
    "        query = self.query(x)\n",
    "        keys = self.keys(x)\n",
    "        values = self.values(x)\n",
    "\n",
    "        query = query.view(b, s, self.q_heads, self.head_dim)\n",
    "        keys = keys.view(b, s, self.kv_heads, self.head_dim)\n",
    "        values = values.view(b, s, self.kv_heads, self.head_dim)\n",
    "        \n",
    "        query = query.transpose(1, 2)\n",
    "        query = query.view(b, self.q_heads//self.kv_heads, self.kv_heads, s, self.head_dim)\n",
    "        keys = keys.transpose(1, 2).unsqueeze(1)\n",
    "        values = values.transpose(1, 2)\n",
    "        \n",
    "        atten_score = torch.sum(query@keys.transpose(3,4), dim=1)\n",
    "        out = atten_score@values\n",
    "        out = out.transpose(1,2).contiguous().view(b, s, self.kv_heads*self.head_dim)\n",
    "        return self.output(out)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "17538377-f3c9-474a-b5a6-7ff9e45e1ee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 128, 256])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GroupQueryAttention(256, 8, 2)\n",
    "x = torch.randn((16, 128, 256))\n",
    "out = model(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208f3762-2e6e-4e81-b53a-255c51386564",
   "metadata": {},
   "source": [
    "## Multi Query Attention\n",
    "It is the fastest among the above attention mechanisms but comes with a trade off of quality. It was introduced in the paper [Shazeer (2019), \"Fast Transformer Decoding: One Write-Head is All You Need\"](https://arxiv.org/abs/1911.02150)\n",
    "It is a special case of Group Query Attention where the number of keys heads and value heads are equal to 1.\n",
    "![Multi Query Attention](./assets/MQA.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f17a8b7b-96ef-4428-86a9-2ea299f0fc36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 128, 256])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GroupQueryAttention(256, 8, 1)\n",
    "x = torch.randn(16, 128, 256)\n",
    "out = model(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134f4483-b055-4152-9a33-2794c1046d00",
   "metadata": {},
   "source": [
    "# Multi Head Latent Attention\n",
    "Multi‐Head Latent Attention (MLA). MLA was introduced in the DeepSeek‑V2 paper [arxiv.org](https://arxiv.org/pdf/2405.04434) as a variant of standard multi‑head attention that reduces the key–value (KV) cache size by first **compressing** the KV representations into a latent (low‑dimensional) space and then **decompressing** them per head. The original implementation contains **Rotary Positional Encodings (RoPE)** but for simplicity we won't be using that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23562bef-b9d9-4e58-ba2a-02c001f5ecc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class MultiHeadLatentAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_latent):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_head = d_model // num_heads\n",
    "\n",
    "        self.query_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Instead of separate full-dim key and value projections,\n",
    "        # we first compress the input into a latent space.\n",
    "        self.kv_down_proj = nn.Linear(d_model, d_latent)\n",
    "        # Then, for each head, we up-project the latent vector into keys and values.\n",
    "        self.k_up_proj = nn.Linear(d_latent, d_model)\n",
    "        self.v_up_proj = nn.Linear(d_latent, d_model)\n",
    "\n",
    "        # Final output projection.\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # x: (batch, seq_len, d_model)\n",
    "        B, T, _ = x.size()\n",
    "\n",
    "        # Compute queries normally.\n",
    "        Q = self.query_proj(x)  # (B, T, d_model)\n",
    "\n",
    "        # Compress x to latent space for keys and values.\n",
    "        latent = self.kv_down_proj(x)  # (B, T, d_latent)\n",
    "\n",
    "        # Up-project latent to full key and value representations.\n",
    "        K = self.k_up_proj(latent)  # (B, T, d_model)\n",
    "        V = self.v_up_proj(latent)  # (B, T, d_model)\n",
    "\n",
    "        Q = Q.view(B, T, self.num_heads, self.d_head).transpose(1, 2)\n",
    "        K = K.view(B, T, self.num_heads, self.d_head).transpose(1, 2)\n",
    "        V = V.view(B, T, self.num_heads, self.d_head).transpose(1, 2)\n",
    "\n",
    "        # Scaled dot-product attention.\n",
    "        # (B, num_heads, T, T)\n",
    "        attn_logits = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_head)\n",
    "        if mask is not None:\n",
    "            attn_logits = attn_logits.masked_fill(mask == 0, -1e9)\n",
    "        attn_weights = F.softmax(attn_logits, dim=-1)\n",
    "        context = torch.matmul(attn_weights, V)  # (B, num_heads, T, d_head)\n",
    "\n",
    "        # Concatenate heads and project.\n",
    "        context = context.transpose(1, 2).contiguous().view(B, T, self.d_model)\n",
    "        out = self.out_proj(context)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f48b0272-773e-4a9b-8f7d-21db0d572df8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 128, 256])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn((16, 128, 256))\n",
    "model = MultiHeadLatentAttention(256, 8, 64)\n",
    "out = model(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf91770-51f6-4778-ab77-c0e2d6b7ac50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
